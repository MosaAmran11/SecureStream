{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dceb9f2",
   "metadata": {},
   "source": [
    "# Live network threat detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c207d6",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "from cicflowmeter.sniffer import create_sniffer\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10f4ab",
   "metadata": {},
   "source": [
    "### Define the directory to watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7eab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the traffic file\n",
    "traffic_file_path = '../traffic_data/flows.csv'\n",
    "# Define the path to the file where the last processed position or timestamp is stored\n",
    "checkpoint_file_path = '../traffic_data/last_processed_checkpoint.ckpt'\n",
    "# Define the path to the anomaly history file\n",
    "anomaly_history_file = '../traffic_data/anomaly_history.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "# INTERFACE = \"eth5\"\n",
    "# LOG_FILE = \"../traffic_data/cicflowmeter_output.log\"\n",
    "\n",
    "# def start_capturing():\n",
    "#     # 1. Setup Logging (Equivalent to: -v > ... 2>&1)\n",
    "#     # We configure the logger to write to the file instead of the console\n",
    "#     logging.basicConfig(\n",
    "#         filename=LOG_FILE,\n",
    "#         level=logging.DEBUG, # Equivalent to -v (verbose)\n",
    "#         format='%(asctime)s | %(levelname)s | %(message)s',\n",
    "#         filemode='w' # Overwrite mode (use 'a' to append)\n",
    "#     )\n",
    "    \n",
    "#     # Also redirect standard stdout/stderr to the log file to capture \n",
    "#     # any raw print statements or crash errors\n",
    "#     sys.stdout = open(LOG_FILE, 'a')\n",
    "#     sys.stderr = sys.stdout\n",
    "\n",
    "#     print(f\"Starting CICFlowMeter on {INTERFACE}...\")\n",
    "\n",
    "#     # 2. Create and Start the Sniffer\n",
    "#     try:\n",
    "#         # Equivalent to: cicflowmeter -i wlan0 -c traffic_data/flows.csv\n",
    "#         sniffer = create_sniffer(\n",
    "#             input_file=None,    # None means live capture\n",
    "#             input_interface=INTERFACE,\n",
    "#             output_mode='flow',     # Ensure we are outputting flow data\n",
    "#             output=traffic_file_path, \n",
    "#             verbose=True            # Helper for internal library logging\n",
    "#         )\n",
    "        \n",
    "#         # This will run indefinitely until stopped (Ctrl+C or kill signal)\n",
    "#         # sniffer.start()\n",
    "\n",
    "#     except PermissionError:\n",
    "#         logging.error(\"Permission denied. You must run this script with sudo.\")\n",
    "#     except Exception as e:\n",
    "#         logging.exception(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625c812",
   "metadata": {},
   "source": [
    "### Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd92faa",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "model = joblib.load('../models/rf_classifier.pkl')  # Change 'your_trained_model.pkl' to the path of your trained model file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebe8bb8",
   "metadata": {},
   "source": [
    "### Function to read the last processed position or timestamp from the checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dac2ba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def read_checkpoint():\n",
    "    \"\"\"\n",
    "    Read the last processed position or timestamp from the checkpoint file.\n",
    "\n",
    "    Returns:\n",
    "        int: Last processed position or timestamp.\n",
    "    \"\"\"\n",
    "    if os.path.exists(checkpoint_file_path):\n",
    "        with open(checkpoint_file_path, 'r') as f:\n",
    "            try:\n",
    "                return int(f.read())\n",
    "            except ValueError:\n",
    "                return 1104\n",
    "    else:\n",
    "        return 1104  # Start from the beginning of the file if checkpoint file doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921123f",
   "metadata": {},
   "source": [
    "### Function to write the last processed position or timestamp to the checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d011e1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def write_checkpoint(position):\n",
    "    \"\"\"\n",
    "    Write the last processed position or timestamp to the checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        position (int): Last processed position or timestamp.\n",
    "    \"\"\"\n",
    "    with open(checkpoint_file_path, 'w') as f:\n",
    "        f.write(str(position))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688e58c9",
   "metadata": {},
   "source": [
    "### Function to write anomalies to the anomaly history CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1567cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_anomalies_to_csv(anomalies):\n",
    "    \"\"\"\n",
    "    Write anomalies to the anomaly history CSV file.\n",
    "\n",
    "    Args:\n",
    "        anomalies (list): List of anomaly information dictionaries.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(anomaly_history_file):\n",
    "        with open(anomaly_history_file, 'w') as f:\n",
    "            f.write(\"Timestamp,Anomaly\\n\")  # Write header if file doesn't exist\n",
    "    df = pd.DataFrame(anomalies)\n",
    "    df.to_csv(anomaly_history_file, mode='a', index=False, header=False)  # Append to file without writing header again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb02c07",
   "metadata": {},
   "source": [
    "### Define function to preprocess data and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8d100",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def predict_anomalies(new_data):\n",
    "    \"\"\"\n",
    "    Predict anomalies in the new data.\n",
    "\n",
    "    Args:\n",
    "        new_data (DataFrame): New data to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "        list: List of anomaly information dictionaries.\n",
    "    \"\"\"\n",
    "    # Preprocess the new data\n",
    "    df = new_data.drop(columns=['destination_port', 'protocol', 'timestamp', 'source_ip', 'destination_ip', 'source_port', 'cwr_flag_count']).sort_index(axis=1)\n",
    "\n",
    "    # Make predictions on the new data\n",
    "    predictions = model.predict(df)  # Assuming 'label' is the target column and is not included in the features\n",
    "    \n",
    "    # Initialize list to store anomalies\n",
    "    anomalies = []\n",
    "\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if prediction != 0:\n",
    "            labelEncoder = joblib.load('../models/label_encoder.joblib')\n",
    "            anomaly_info = {\n",
    "                \"timestamp\": new_data.loc[i, 'timestamp'],\n",
    "                \"anomaly\": labelEncoder.inverse_transform([prediction])[0]\n",
    "            }\n",
    "            anomalies.append(anomaly_info)\n",
    "            print(f\"ðŸ”´ Anomaly detected: {anomaly_info['anomaly']} at {anomaly_info['timestamp']}\")\n",
    "    if anomalies == []:\n",
    "        print(\"ðŸŸ¢ No anomalies detected\")\n",
    "\n",
    "    return anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e7cda8",
   "metadata": {},
   "source": [
    "### Define the event handler for the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9fb3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHandler(FileSystemEventHandler):\n",
    "    \"\"\"\n",
    "    Handler class to detect file modifications and trigger anomaly detection.\n",
    "    \"\"\"\n",
    "    def on_modified(self, event):\n",
    "        \"\"\"\n",
    "        Method called when a file is modified.\n",
    "        \"\"\"\n",
    "        if event.src_path == traffic_file_path:\n",
    "            print(\"File modified. Detecting anomalies...\")\n",
    "            \n",
    "            # Read the last processed position from the checkpoint file\n",
    "            last_processed_position = read_checkpoint()\n",
    "            \n",
    "            # Read the new data from the traffic file, starting from the last processed position\n",
    "            with open(traffic_file_path, 'r') as f:\n",
    "                header = f.readline().strip('\\n').split(',')\n",
    "\n",
    "                # Move the file pointer to the last processed position\n",
    "                f.seek(last_processed_position)\n",
    "\n",
    "                # Read the data from the file, starting from the last processed position\n",
    "                data = f.readlines()\n",
    "\n",
    "                # Combine header with data\n",
    "                combined_data = [row.strip('\\n').split(',') for row in data]\n",
    "\n",
    "                # Create a DataFrame from the combined data\n",
    "                new_data = pd.DataFrame(combined_data, columns=header)\n",
    "\n",
    "                # Retrieve the current file position\n",
    "                current_position = f.tell()\n",
    "            \n",
    "            # Trigger prediction function\n",
    "            try:\n",
    "                anomalies = predict_anomalies(new_data)\n",
    "                if anomalies:\n",
    "                    write_anomalies_to_csv(anomalies)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "            # Update the last processed position in the checkpoint file\n",
    "            write_checkpoint(current_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a448ce3",
   "metadata": {},
   "source": [
    "### Set up file system event observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "event_handler = MyHandler()\n",
    "observer = Observer()\n",
    "observer.schedule(event_handler, path=traffic_file_path, recursive=False)\n",
    "observer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12fea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    observer.stop()\n",
    "observer.join()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
